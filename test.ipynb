{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T06:47:57.859306800Z",
     "start_time": "2023-05-11T06:47:56.944304800Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy.io as sciio\n",
    "from time import time\n",
    "import sklearn\n",
    "# %matplotlib inline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage\n",
    "from sklearn import svm, metrics, datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearnex import patch_sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.transform import resize as imresize\n",
    "\n",
    "from load_data import LOAD_IMAGE\n",
    "from PLOT import plot_digits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 调参部分"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "patch_sklearn()\n",
    "\n",
    "REAL_IMAG = True\n",
    "NO_NMF = False\n",
    "is_NMF = False\n",
    "SCALED = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T08:02:00.654599100Z",
     "start_time": "2023-05-11T08:02:00.642614900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据加载部分"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "3 4\n",
      "4 5\n",
      "0 10\n",
      "1 6\n",
      "2 7\n",
      "3 8\n",
      "4 9\n",
      "fucked\n",
      "Data load done in 105.099s\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "# ----KC501-----\n",
    "# image_dataset = load_image_files('/home/kc501/LJY/Alexnet/dataRCS')\n",
    "\n",
    "# ----ICraft----\n",
    "t_load_data = time()\n",
    "image_dataset = LOAD_IMAGE(REAL_IMAG)\n",
    "# image_dataset1 = load_image_files('.\\dataRCS')\n",
    "# image_dataset2 = load_image_files('.\\dataRCS2')\n",
    "# # flat_data = np.concatenate([image_dataset1.data, image_dataset2.data])\n",
    "# # target = np.concatenate([image_dataset1.target, image_dataset2.target])\n",
    "# # categories = np.concatenate([image_dataset1.categories, image_dataset2.categories])\n",
    "# # descr = image_dataset1.descr\n",
    "# categories = image_dataset1.target_names\n",
    "# categories.extend(image_dataset2.target_names)\n",
    "# print(\"fucked\")\n",
    "# image_dataset = Bunch(data=np.concatenate([image_dataset1.data, image_dataset2.data]),\n",
    "#                       target=np.concatenate([image_dataset1.target, image_dataset2.target]),\n",
    "#                       target_names=categories,\n",
    "#                       DESCR=image_dataset1.DESCR)\n",
    "print(\"Data load done in %0.3fs\" % (time() - t_load_data))\n",
    "# image_dataset_test = load_image_files(\"E:/RL_code/alex-net-image-classification-master/class3/val\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T06:49:42.984474500Z",
     "start_time": "2023-05-11T06:47:57.875253400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据集划分部分"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T08:30:05.252851800Z",
     "start_time": "2023-05-11T08:29:56.500222800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_dataset.data, image_dataset.target, test_size=0.2, random_state=101)\n",
    "\n",
    "if SCALED:\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NMF降维模块"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T08:31:41.193509700Z",
     "start_time": "2023-05-11T08:30:09.605850400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA done in 91.570s\n"
     ]
    }
   ],
   "source": [
    "n_components = 0.95\n",
    "tolerance = 1e-2\n",
    "max_iteration = 10000\n",
    "if not NO_NMF:\n",
    "    t_NMF = time()\n",
    "    if is_NMF:\n",
    "        nmf = NMF(n_components=n_components, init='random', tol=tolerance, max_iter=max_iteration, verbose=True).fit(X_train)\n",
    "        # nmf = NMF(n_components=n_components, init='random', tol=tolerance, max_iter=max_iteration, verbose=True).fit(\n",
    "        #     image_dataset.data)\n",
    "        print(\"NMF done in %0.3fs\" % (time() - t_NMF))\n",
    "        X_train_nmf = nmf.transform(X_train)\n",
    "        X_test_nmf = nmf.transform(X_test)\n",
    "    else:\n",
    "        nmf = PCA(n_components=n_components).fit(X_train)\n",
    "        # nmf =  PCA(n_components).fit(image_dataset.data)\n",
    "        # pca_tsne = Pipeline([\n",
    "        #     ('pca', PCA(n_components=n_components)),\n",
    "        #     ('tsne', TSNE(n_components=3))\n",
    "        # ])\n",
    "        # X_train_nmf = pca_tsne.fit_transform(X_train)\n",
    "        # X_test_nmf = pca_tsne.fit_transform(X_test)\n",
    "        X_train_nmf = nmf.transform(X_train)\n",
    "        X_test_nmf = nmf.transform(X_test)\n",
    "        print(\"PCA done in %0.3fs\" % (time() - t_NMF))\n",
    "        # plot_digits(X_train_nmf, y_train)\n",
    "        # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM训练模块\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T08:40:48.283100700Z",
     "start_time": "2023-05-11T08:40:47.635232600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]SVM with scaled done in 0.168s\n",
      "n_components:  0.95\n",
      "Acc on train data:0.9375\n",
      "Acc on test data:0.554\n"
     ]
    }
   ],
   "source": [
    "f = open('./TZB.txt', 'a')\n",
    "\n",
    "def SVM(clf, X_train, X_test):\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"SVM with scaled done in %0.3fs\" % (time() - t_train))\n",
    "    f.write(\"SVM with scaled done in %0.3fs\\n\" % (time() - t_train))\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    return y_pred_train, y_pred_test\n",
    "\n",
    "C=100\n",
    "t_train = time()\n",
    "kernel = 'rbf'\n",
    "clf = SVC(C=C, kernel=kernel, gamma='scale', class_weight='balanced', decision_function_shape='ovo', verbose=True)\n",
    "\n",
    "if NO_NMF:\n",
    "    y_pred_train, y_pred_test = SVM(clf, X_train, X_test)\n",
    "else:\n",
    "    y_pred_train, y_pred_test = SVM(clf, X_train_nmf, X_test_nmf)\n",
    "\n",
    "y_true_train = y_train\n",
    "y_true_test = y_test\n",
    "\n",
    "f.write(\"n_components: {}\\n\".format(n_components))\n",
    "f.write(\"kernel: {}\\n\".format(kernel))\n",
    "f.write(\"Decomposition: {}\\n\".format(\"NMF\" if is_NMF else \"PCA\"))\n",
    "f.write(\"tolerance: {}\\n\".format(tolerance))\n",
    "f.write(\"max_iteration: {}\\n\".format(max_iteration))\n",
    "f.write(\"C: {}\\n\".format(C))\n",
    "f.write(\"Acc on train data: {}\\n\".format(accuracy_score(y_true_train, y_pred_train)))\n",
    "f.write(\"Acc on test data: {}\\n\".format(accuracy_score(y_true_test, y_pred_test)))\n",
    "f.write(\"------------------------------------------------------------------------------\\n\")\n",
    "f.close()\n",
    "print(\"n_components: \", n_components)\n",
    "print(\"Acc on train data:\" + str(accuracy_score(y_true_train, y_pred_train)))\n",
    "print(\"Acc on test data:\" + str(accuracy_score(y_true_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
