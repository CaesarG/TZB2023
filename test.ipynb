{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T06:12:08.980763200Z",
     "start_time": "2023-05-06T06:12:07.777938100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy.io as sciio\n",
    "from time import time\n",
    "import sklearn\n",
    "# %matplotlib inline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.svm import SVC\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage\n",
    "from sklearn import svm, metrics, datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.transform import resize as imresize\n",
    "\n",
    "REAL_IMAG = True\n",
    "NO_NMF = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "3 4\n",
      "4 5\n",
      "0 10\n",
      "1 6\n",
      "2 7\n",
      "3 8\n",
      "4 9\n",
      "fucked\n",
      "Data load done in 204.128s\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "# def Angle(data):\n",
    "#     ans = []\n",
    "#     for i in data:\n",
    "#         # x=np.arctan(np.imag(i)/np.real(i))\n",
    "#         # ans.append(np.arctan(np.imag(i)/np.real(i)))\n",
    "#         ans.append(np.angle(i))\n",
    "#     return ans\n",
    "\n",
    "\n",
    "# Load images in structured directory like it's sklearn sample dataset\n",
    "def load_image_files(container_path, dimension=(64, 64)):  # 调整图片的尺寸为dimension=(64, 64)\n",
    "\n",
    "    image_dir = Path(container_path)\n",
    "    # folders is the list of folders each conains a category of data\n",
    "    folders = [directory for directory in image_dir.iterdir() if directory.is_dir()]\n",
    "    # _______________________________________________________\n",
    "    # 去掉annotations文件夹\n",
    "    # folders = folders[:-1]\n",
    "    # _______________________________________________________\n",
    "    categories = [fo.name for fo in folders]\n",
    "\n",
    "    descr = \"A image classification dataset\"\n",
    "    images = []\n",
    "    flat_data = []\n",
    "    target = []\n",
    "\n",
    "    for i, direc in enumerate(folders):\n",
    "        # print(i)\n",
    "        print(i, categories[i])\n",
    "        # print(direc)\n",
    "        for file in direc.iterdir():\n",
    "            mat_data = sciio.loadmat(file)\n",
    "            # img = np.abs(mat_data['frame_Ev'])\n",
    "            raw_data = []\n",
    "            if REAL_IMAG:\n",
    "                # raw_data.extend(np.real(mat_data['frame_Ev']).flatten())\n",
    "                # raw_data.extend(np.imag(mat_data['frame_Ev']).flatten())\n",
    "                # raw_data.extend(np.real(mat_data['frame_Eh']).flatten())\n",
    "                # raw_data.extend(np.imag(mat_data['frame_Eh']).flatten())\n",
    "                # raw_data = np.abs(raw_data)\n",
    "                raw_data.extend(np.abs(mat_data['frame_Ev']).flatten())\n",
    "                # raw_data.extend((np.angle(mat_data['frame_Ev'].flatten())))\n",
    "                raw_data.extend(((np.angle(mat_data['frame_Ev'].flatten()) / np.pi + 1) * 100))\n",
    "                raw_data.extend(np.abs(mat_data['frame_Eh'].flatten()))\n",
    "                # raw_data.extend((np.angle(mat_data['frame_Eh']).flatten()))\n",
    "                raw_data.extend(((np.angle(mat_data['frame_Eh'].flatten()) / np.pi + 1) * 100))\n",
    "                # print(Angle(mat_data['frame_Ev'].flatten()))\n",
    "                # break\n",
    "            else:\n",
    "                raw_data.extend(np.abs(mat_data['frame_Ev']).flatten())\n",
    "                raw_data.extend(np.abs(mat_data['frame_Eh']).flatten())\n",
    "            flat_data.append(raw_data)\n",
    "            # images.append(img)\n",
    "            target.append(categories[i])\n",
    "        # break\n",
    "    # return [flat_data,target,categories,descr]\n",
    "    flat_data = np.array(flat_data)\n",
    "    target = np.array(target)\n",
    "    return Bunch(data=flat_data,\n",
    "                 target=target,\n",
    "                 target_names=categories,\n",
    "                 DESCR=descr)\n",
    "\n",
    "\n",
    "def LOAD_IMAGE():\n",
    "    image_dataset1 = load_image_files('.\\dataRCS')\n",
    "    image_dataset2 = load_image_files('.\\dataRCS2')\n",
    "    # flat_data = np.concatenate([image_dataset1.data, image_dataset2.data])\n",
    "    # target = np.concatenate([image_dataset1.target, image_dataset2.target])\n",
    "    # categories = np.concatenate([image_dataset1.categories, image_dataset2.categories])\n",
    "    # descr = image_dataset1.descr\n",
    "    categories = image_dataset1.target_names\n",
    "    categories.extend(image_dataset2.target_names)\n",
    "    print(\"fucked\")\n",
    "    return Bunch(data=np.concatenate([image_dataset1.data, image_dataset2.data]),\n",
    "                 target=np.concatenate([image_dataset1.target, image_dataset2.target]),\n",
    "                 target_names=categories,\n",
    "                 DESCR=image_dataset1.DESCR)\n",
    "\n",
    "\n",
    "# ----KC501-----\n",
    "# image_dataset = load_image_files('/home/kc501/LJY/Alexnet/dataRCS')\n",
    "\n",
    "# ----ICraft----\n",
    "t_load_data = time()\n",
    "image_dataset = LOAD_IMAGE()\n",
    "# image_dataset1 = load_image_files('.\\dataRCS')\n",
    "# image_dataset2 = load_image_files('.\\dataRCS2')\n",
    "# # flat_data = np.concatenate([image_dataset1.data, image_dataset2.data])\n",
    "# # target = np.concatenate([image_dataset1.target, image_dataset2.target])\n",
    "# # categories = np.concatenate([image_dataset1.categories, image_dataset2.categories])\n",
    "# # descr = image_dataset1.descr\n",
    "# categories = image_dataset1.target_names\n",
    "# categories.extend(image_dataset2.target_names)\n",
    "# print(\"fucked\")\n",
    "# image_dataset = Bunch(data=np.concatenate([image_dataset1.data, image_dataset2.data]),\n",
    "#                       target=np.concatenate([image_dataset1.target, image_dataset2.target]),\n",
    "#                       target_names=categories,\n",
    "#                       DESCR=image_dataset1.DESCR)\n",
    "print(\"Data load done in %0.3fs\" % (time() - t_load_data))\n",
    "# image_dataset_test = load_image_files(\"E:/RL_code/alex-net-image-classification-master/class3/val\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T06:15:35.175361400Z",
     "start_time": "2023-05-06T06:12:11.032798200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T06:16:48.706692700Z",
     "start_time": "2023-05-06T06:16:45.582764800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_dataset.data, image_dataset.target, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-06T11:25:13.870950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violation: 1.0\n",
      "violation: 0.415232152475027\n",
      "violation: 0.328275720935333\n",
      "violation: 0.2951128614168521\n",
      "violation: 0.277334130957208\n",
      "violation: 0.26375973425707244\n",
      "violation: 0.25296562881321866\n",
      "violation: 0.24296488353214923\n",
      "violation: 0.23429389003810752\n",
      "violation: 0.22590803163311945\n",
      "violation: 0.2197988908177308\n",
      "violation: 0.2123233367838285\n",
      "violation: 0.2067661630929283\n",
      "violation: 0.20138332464104772\n",
      "violation: 0.1965566504981698\n",
      "violation: 0.1921939186915778\n",
      "violation: 0.18907546876511871\n",
      "violation: 0.18585473916377004\n",
      "violation: 0.18192699880398835\n",
      "violation: 0.1783070432938446\n",
      "violation: 0.17511291781779498\n",
      "violation: 0.17186517096743695\n",
      "violation: 0.16868705698433914\n",
      "violation: 0.16572006188846636\n",
      "violation: 0.1627726645838626\n",
      "violation: 0.1595327704458579\n"
     ]
    }
   ],
   "source": [
    "n_components = 100\n",
    "tolerance = 1e-2\n",
    "max_iteration = 10000\n",
    "if not NO_NMF:\n",
    "    t_NMF = time()\n",
    "    nmf = NMF(n_components=n_components, init='nndsvd', tol=tolerance, max_iter=max_iteration, verbose=True).fit(\n",
    "        X_train)\n",
    "    # W = nmf.components_.reshape((n_components, 64, 64))\n",
    "\n",
    "    X_train_nmf = nmf.transform(X_train)\n",
    "    X_test_nmf = nmf.transform(X_test)\n",
    "    print(\"NMF done in %0.3fs\" % (time() - t_NMF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T11:20:58.554417300Z",
     "start_time": "2023-05-06T11:20:55.516491400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM done in 0.000s\n",
      "[LibSVM]n_components:  20\n",
      "Acc on train data:0.9585\n",
      "Acc on test data:0.238\n"
     ]
    }
   ],
   "source": [
    "f = open('./TZB.txt', 'a')\n",
    "t_train = time()\n",
    "kernel = 'poly'\n",
    "clf = SVC(C=1, kernel=kernel, gamma=0.01, class_weight='balanced', decision_function_shape='ovo', verbose=True)\n",
    "print(\"SVM done in %0.3fs\" % (time() - t_train))\n",
    "\n",
    "if NO_NMF:\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "else:\n",
    "    clf = clf.fit(X_train_nmf, y_train)\n",
    "    y_pred_train = clf.predict(X_train_nmf)\n",
    "    y_pred_test = clf.predict(X_test_nmf)\n",
    "\n",
    "y_true_train = y_train\n",
    "y_true_test = y_test\n",
    "\n",
    "f.write(\"n_components: {}\\n\".format(n_components))\n",
    "f.write(\"kernel: {}\\n\".format(kernel))\n",
    "f.write(\"tolerance: {}\\n\".format(tolerance))\n",
    "f.write(\"max_iteration: {}\\n\".format(max_iteration))\n",
    "f.write(\"Acc on train data: {}\\n\".format(accuracy_score(y_true_train, y_pred_train)))\n",
    "f.write(\"Acc on test data: {}\\n\".format(accuracy_score(y_true_test, y_pred_test)))\n",
    "f.write(\"------------------------------------------------------------------------------\\n\")\n",
    "f.close()\n",
    "print(\"n_components: \", n_components)\n",
    "print(\"Acc on train data:\" + str(accuracy_score(y_true_train, y_pred_train)))\n",
    "\n",
    "print(\"Acc on test data:\" + str(accuracy_score(y_true_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
